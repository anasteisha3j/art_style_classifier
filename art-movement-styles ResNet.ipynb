{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4804396,"sourceType":"datasetVersion","datasetId":2779739}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"sivarazadi/wikiart-art-movementsstyles\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:15:23.251059Z","iopub.execute_input":"2025-11-17T13:15:23.251320Z","iopub.status.idle":"2025-11-17T13:19:01.587799Z","shell.execute_reply.started":"2025-11-17T13:15:23.251292Z","shell.execute_reply":"2025-11-17T13:19:01.586796Z"}},"outputs":[{"name":"stdout","text":"Mounting files to /kaggle/input/wikiart-art-movementsstyles...\nPath to dataset files: /kaggle/input/wikiart-art-movementsstyles\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom tqdm.notebook import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:07:03.989347Z","iopub.execute_input":"2025-11-20T13:07:03.990005Z","iopub.status.idle":"2025-11-20T13:07:03.997354Z","shell.execute_reply.started":"2025-11-20T13:07:03.989964Z","shell.execute_reply":"2025-11-20T13:07:03.995863Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 32\n\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor()\n])\n\ndata_dir = \"/kaggle/input/wikiart-art-movementsstyles\"\n\ntrain_dataset = ImageFolder(data_dir, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nnum_classes = len(train_dataset.classes)\n\nprint(\"Classes:\", train_dataset.classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:09:35.428390Z","iopub.execute_input":"2025-11-20T13:09:35.428786Z","iopub.status.idle":"2025-11-20T13:09:54.604557Z","shell.execute_reply.started":"2025-11-20T13:09:35.428757Z","shell.execute_reply":"2025-11-20T13:09:54.603472Z"}},"outputs":[{"name":"stdout","text":"Classes: ['Academic_Art', 'Art_Nouveau', 'Baroque', 'Expressionism', 'Japanese_Art', 'Neoclassicism', 'Primitivism', 'Realism', 'Renaissance', 'Rococo', 'Romanticism', 'Symbolism', 'Western_Medieval']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:12:25.293132Z","iopub.execute_input":"2025-11-20T13:12:25.293511Z","iopub.status.idle":"2025-11-20T13:12:25.557928Z","shell.execute_reply.started":"2025-11-20T13:12:25.293487Z","shell.execute_reply":"2025-11-20T13:12:25.556823Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:12:32.682594Z","iopub.execute_input":"2025-11-20T13:12:32.683013Z","iopub.status.idle":"2025-11-20T13:12:32.692339Z","shell.execute_reply.started":"2025-11-20T13:12:32.682984Z","shell.execute_reply":"2025-11-20T13:12:32.691180Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"EPOCHS = 3\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n\n    for images, labels in tqdm(train_loader):\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:12:36.267373Z","iopub.execute_input":"2025-11-20T13:12:36.267694Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1329 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08ba8424c6cf41f8ad1be7fb5271fb1c"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (96714256 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 2592.9401\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1329 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74a6782b0ee401e86809241504e2c46"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"art_style_resnet18.pth\")\nprint(\"Model saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import PIL.Image as Image\n\ndef predict_image(path):\n    img = Image.open(path).convert(\"RGB\")\n    img = transform(img).unsqueeze(0).to(device)\n\n    model.eval()\n    with torch.no_grad():\n        output = model(img)\n        pred = output.argmax(dim=1).item()\n\n    return train_dataset.classes[pred]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- EXAMPLE: TEST ON ONE IMAGE ----------\n# Replace path with any image from your dataset\nexample_path = list(train_data.filepaths)[0]\nstyle, conf = predict_style(example_path)\n\nprint(\"Example prediction:\")\nprint(\"Style:\", style)\nprint(\"Confidence:\", conf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  GRADIO interface\nimport gradio as gr\n\ndef classify_image(img):\n    img = img.resize((IMG_SIZE, IMG_SIZE))\n    img_array = np.array(img) / 255.0\n    img_array = np.expand_dims(img_array, axis=0)\n\n    preds = model.predict(img_array)\n    class_index = np.argmax(preds)\n    class_name = list(train_data.class_indices.keys())[class_index]\n    confidence = float(preds[0][class_index])\n\n    return {class_name: confidence}\n\ndemo = gr.Interface(\n    fn=classify_image,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=gr.Label(),\n    title=\"Art Style Classifier\"\n)\n\ndemo.launch(debug=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}